# -*- coding: utf-8 -*-
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from forum.items import PostItemsList
from scrapy.selector import Selector
import logging
import re
from bs4 import BeautifulSoup

class CancerForum(CrawlSpider):
    name = "breastcancer_nosurrender_spider"
    allowed_domains = ["nosurrenderbreastcancersupportforum.com"]
    start_urls = [
        "http://www.nosurrenderbreastcancersupportforum.com/"
    ]

    rules = (
        Rule(LinkExtractor(restrict_xpaths='//td[contains(@valign,"top")]/table[contains(@class,"tables")]//a[contains(@class,"forum")]'
            ,canonicalize=True),
             callback="parsePostsList", follow=True),

        Rule(LinkExtractor(restrict_xpaths='id("main_container")/div[2]/form[1]/table/tbody/tr[1]/td/table/tbody/tr/td[2]/a/@href',canonicalize=True),
             callback='parsePostsList', follow=True),
    )

    def cleanText(self,text):
        soup = BeautifulSoup(text,'html.parser')
        text = soup.get_text();
        text = re.sub("( +|\n|\r|\t|\0|\x0b|\xa0|\xbb|\xab)+",' ',text).strip()
        return text 
    
    def parsePostsList(self,response):
        sel = Selector(response)
        html = response.body
        soup = BeautifulSoup(html)
        condition = "breast cancer"
        try:

            users = soup.findAll('a',{'class':re.compile('usergroup\d.*')})
            items = []
            topic = response.xpath('//tbody/tr[2]/td[2]/table/tbody/tr[1]/td/div/b').extract()
            url = response.url
            for x in range(len(users)):
                item = PostItemsList()
                item['author'] = users[x].text
                item['author_link']=users[x]['href']
                item['condition'] = condition
                item['create_date']= soup.findAll('span',{'id':re.compile('posted_date_.*')})[x].text
                item['post'] = soup.findAll('span',{'id':re.compile('post_message.*')})[x].text
                item['tag']='cancer'
                item['topic'] = topic
                item['url']=url
                items.append(item)
            return items
        except Exception:
            logging.error("faile")